# Release Notes: RLM-Inspired Context Improvements

**Version**: 0.15.0 (Feature Enhancement)
**Date**: 2026-01-18
**Cycle**: cycle-002 (RLM-Inspired Context Improvements)

---

## Overview

This release implements RLM (Relevance-based Loading Method) patterns in Loa's context management, achieving significant token reduction for large codebases while maintaining accuracy.

## Key Features

### Probe-Before-Load Pattern

The new probe functionality allows agents to assess files before loading them into context:

```bash
# Probe a file
.claude/scripts/context-manager.sh probe src/index.ts --json

# Check if a file should be loaded
.claude/scripts/context-manager.sh should-load large-file.ts --json
```

**Benefits**:
- 29.3% token reduction on Loa codebase
- Minimal overhead (0.6% of total context)
- Transparent decision-making via JSON output

### Schema Validator Assertions

Programmatic validation of structured outputs without re-prompting:

```bash
# Validate with assertions
.claude/scripts/schema-validator.sh assert prd.md --schema prd --json
```

**Supported assertions**:
- Field existence checks
- Enum value validation
- Semver format validation
- Array non-empty checks

### RLM Benchmark Framework

New benchmarking infrastructure for measuring context efficiency:

```bash
# Run benchmark
.claude/scripts/rlm-benchmark.sh run --target . --json

# Create baseline
.claude/scripts/rlm-benchmark.sh baseline

# Compare against baseline
.claude/scripts/rlm-benchmark.sh compare

# Generate report
.claude/scripts/rlm-benchmark.sh report
```

## Configuration

New configuration options in `.loa.config.yaml`:

```yaml
context_management:
  probe_before_load: true
  max_eager_load_lines: 500
  relevance_keywords: ["export", "class", "interface", "function"]
  exclude_patterns: ["*.test.ts", "*.spec.ts", "node_modules/**"]
```

## Performance Metrics

| Metric | Value |
|--------|-------|
| Token Reduction | 29.3% |
| Probe Overhead | 0.6% |
| Single File Probe | ~21ms |
| Directory Probe (371 files) | ~1.8s |

## Test Coverage

- 120 new tests added across 6 sprints
- Total test suite: 887 tests
- All tests passing

## Breaking Changes

None. All changes are additive and backward compatible.

## Migration Guide

No migration required. Features are opt-in via configuration.

To enable probe-before-load:
1. Ensure `context_management.probe_before_load: true` in `.loa.config.yaml`
2. Use new probe commands in your workflows

## Acknowledgments

This implementation is based on research from:
- **Paper**: "Recursive Language Models: Scaling Context Beyond Limits"
- **Authors**: Zhang, Kraska, Khattab
- **Institution**: MIT CSAIL
- **Date**: December 2025

See `grimoires/pub/research/rlm-recursive-language-models.md` for full analysis.

---

## Files Added/Modified

### New Scripts
- `.claude/scripts/rlm-benchmark.sh` - Benchmark framework

### Modified Scripts
- `.claude/scripts/context-manager.sh` - Added probe commands
- `.claude/scripts/schema-validator.sh` - Added assertion mode

### New Test Files
- `tests/unit/context-manager-probe.bats`
- `tests/unit/schema-validator-assert.bats`
- `tests/integration/benchmark-workflow.bats`
- `tests/integration/probe-ride-workflow.bats`
- `tests/edge-cases/context-edge-cases.bats`

### Documentation
- `CLAUDE.md` - Updated with new features
- `grimoires/pub/research/benchmarks/final-report.md` - Benchmark results

---

*Release notes generated by implementing-tasks agent*
